{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews \t# labelled reviews\n",
    "import pickle\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB,BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.svm import SVC,LinearSVC,NuSVC\n",
    "\n",
    "# nltk.download('movie_reviews')\n",
    "\n",
    "# (plot_words,'neg'/'pos')\n",
    "docs = [(list(movie_reviews.words(fileid)),category)\n",
    "\t\tfor category in movie_reviews.categories()\n",
    "\t\tfor fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "random.shuffle(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268\n"
     ]
    }
   ],
   "source": [
    "all_words = []\n",
    "\n",
    "for words in movie_reviews.words():\n",
    "\tall_words.append(words.lower())\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)\t\t# most_common -> least_common\n",
    "\n",
    "# print(all_words.most_common(15))\n",
    "print(all_words[\"stop\"])\n",
    "\n",
    "words_features = list(all_words.keys())[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "def find_features(docum):\n",
    "\twords = set(docum)\n",
    "\tfeatures = {}\n",
    "\tfor word in words_features:\n",
    "\t\tfeatures[word] = word in words\n",
    "\treturn features\n",
    "\n",
    "# print(find_features(movie_reviews.words('neg/cv000_29416.txt')))\n",
    "feature_sets = [(find_features(rev),category) for (rev,category) in docs]\n",
    "\n",
    "print(len(feature_sets))\n",
    "\n",
    "training_set = feature_sets[:1900]    \n",
    "testing_set  = feature_sets[1900:]  \n",
    "\n",
    "#Naive Bayes \n",
    "# print(training_set[:4])\n",
    "# posterior = (prior_occurrences x likelihood) / evidence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original NB accuracy:  81.0\n",
      "Most Informative Features\n",
      "                   sucks = True              neg : pos    =     10.4 : 1.0\n",
      "               ludicrous = True              neg : pos    =     10.3 : 1.0\n",
      "                  annual = True              pos : neg    =      9.8 : 1.0\n",
      "                     ugh = True              neg : pos    =      9.5 : 1.0\n",
      "                 frances = True              pos : neg    =      9.1 : 1.0\n",
      "               stupidity = True              neg : pos    =      8.9 : 1.0\n",
      "              simplicity = True              pos : neg    =      8.5 : 1.0\n",
      "                     dud = True              neg : pos    =      8.2 : 1.0\n",
      "                 wasting = True              neg : pos    =      8.2 : 1.0\n",
      "                 detract = True              pos : neg    =      7.8 : 1.0\n",
      "            wisecracking = True              neg : pos    =      7.6 : 1.0\n",
      "             silverstone = True              neg : pos    =      7.6 : 1.0\n",
      "                  regard = True              pos : neg    =      7.1 : 1.0\n",
      "                narrates = True              pos : neg    =      7.1 : 1.0\n",
      "              schumacher = True              neg : pos    =      6.9 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "# pickling\n",
    "# classifier_f = open('naiveBayes.pickle','rb')\n",
    "# classifier = pickle.load(classifier_f)\n",
    "# classifier_f.close()\n",
    "\n",
    "print(\"Original NB accuracy: \",nltk.classify.accuracy(classifier,testing_set)*100)\n",
    "classifier.show_most_informative_features(15)    # top 15 \n",
    "\n",
    "# save_classifier = open(\"naiveBayes.pickle\",'wb')\n",
    "# pickle.dump(classifier,save_classifier)\n",
    "# save_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB accuracy:  85.0\n",
      "test classification:  pos\n",
      "test label : pos\n",
      "Bernoulli NB accuracy:  81.0\n",
      "Logistic Regression accuracy:  86.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDC accuracy:  77.0\n",
      "SVC accuracy:  72.0\n",
      "LSVC accuracy:  81.0\n",
      "NuSVC accuracy:  87.0\n"
     ]
    }
   ],
   "source": [
    "# using sklearn\n",
    "\n",
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)\n",
    "print(\"Multinomial NB accuracy: \",nltk.classify.accuracy(MNB_classifier,testing_set)*100)\n",
    "\n",
    "# print(testing_set[0])\n",
    "print(\"test classification: \",str(MNB_classifier.classify(testing_set[0][0])))\n",
    "print(\"test label :\",testing_set[0][1])\n",
    "\n",
    "\n",
    "#GNB_classifier = SklearnClassifier(GaussianNB())\n",
    "#GNB_classifier.train(training_set)\n",
    "#print(\"Gaussian NB accuracy: \",nltk.classify.accuracy(GNB_classifier,testing_set)*100)\n",
    "\n",
    "BNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BNB_classifier.train(training_set)\n",
    "print(\"Bernoulli NB accuracy: \",nltk.classify.accuracy(BNB_classifier,testing_set)*100)\n",
    "\n",
    "LogReg_classifier = SklearnClassifier(LogisticRegression())\n",
    "LogReg_classifier.train(training_set)\n",
    "print(\"Logistic Regression accuracy: \",nltk.classify.accuracy(LogReg_classifier,testing_set)*100)\n",
    "\n",
    "\n",
    "SGDC_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGDC_classifier.train(training_set)\n",
    "print(\"SGDC accuracy: \",nltk.classify.accuracy(SGDC_classifier,testing_set)*100)\n",
    "\n",
    "\n",
    "SVC_classifier = SklearnClassifier(SVC())\n",
    "SVC_classifier.train(training_set)\n",
    "print(\"SVC accuracy: \",nltk.classify.accuracy(SVC_classifier,testing_set)*100)\n",
    "\n",
    "\n",
    "LSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "LSVC_classifier.train(training_set)\n",
    "print(\"LSVC accuracy: \",nltk.classify.accuracy(LSVC_classifier,testing_set)*100)\n",
    "\n",
    "\n",
    "NuSVC_classifier = SklearnClassifier(NuSVC())\n",
    "NuSVC_classifier.train(training_set)\n",
    "print(\"NuSVC accuracy: \",nltk.classify.accuracy(NuSVC_classifier,testing_set)*100)\n",
    "\n",
    "# tweak these to get + - 10% accuracy difference \n",
    "# Note : redundancy in data sets consideration ( most common words may contain punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
